{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e30cfa-0106-48e7-9c36-9570ed48636f",
   "metadata": {},
   "source": [
    "## Cамостоятельная реализация решающего дерева  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ef2f9-af35-49fa-9200-4e95cd28973b",
   "metadata": {},
   "source": [
    "#### Определение дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9bb34-c045-4e24-b5c5-622fb5ea66b8",
   "metadata": {},
   "source": [
    "Решающее дерево - **бинарное** дерево, в котором:\n",
    "1. Каждой *внутренней вершине* $v$ присвоен предикат предсказания: $B_v : \\mathbb{X} \\rightarrow \\{0, 1\\}$\n",
    "2. Каждой *листовой вершине* $v$ присвоен прогноз $C_v: \\mathbb{Y}$, где $\\mathbb{Y}$ - область значений таргета\n",
    "\n",
    "Каждый проход дерева начинается из корня. При прохождении очередной вершины мы двигаемся: *вправо*, если $B_v(x) = 1$; *влево*, если $B_v(x) = 0$. \n",
    "\n",
    "При достижении листа на объекте $x$, прогнозом для него будет являться $C_v$\n",
    "\n",
    "Особенности решающего дерева:\n",
    "1. Полученная функция кусочно-постоянная $\\rightarrow$ **не получится применить градиентные методы**\n",
    "2. Дерево **не может экстраполировать данные** за пределы уже имеющейся области значений признаков обучающей выборки\n",
    "3. Дереву свойственно **переобучение**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409d9db-851a-480c-85a8-74c0c9d5173c",
   "metadata": {},
   "source": [
    "#### Решающий пень"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da5e42-69fa-42ef-85e9-df71274f56b4",
   "metadata": {},
   "source": [
    "Дерево можно разбить на составляющие - решающие пни. Они будут представлять собой одну и вершину и два дочерних листа.\n",
    "\n",
    "Вершину мы будем разделять на листы при помощи предиката $B_{j, t}(x_i)$ . Качество разбиения мы будем оценивать при помощи критерия ветвления $Branch$.\n",
    "\n",
    "На листьях подзнее мы можем принять решение о необходимости дальнейшего разбиения -> построения еще одного решающего пня.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be38e5-e706-45b4-8cdb-ecb3d0bb4ed8",
   "metadata": {},
   "source": [
    "#### Сложность решающего пня"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbf3a7-862d-4783-9518-ceb94a548f54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Пусть у нас есть матрица значений признаков $X \\in \\mathbb{R}^{D \\times N}$ и вектор таргетов $Y \\in \\mathbb{R}^N$.\n",
    "\n",
    "В основе вершины пня будет находится разделяющий предикат:\n",
    "$$B_{j, t}(x_i) = \\mathbb{I}\\left[x_{ij} \\le t\\right]$$\n",
    "Мы будем проходить не по самим значениям признаков, а по средним между значениями.\n",
    "$$x_i < t_i \\le x_{i+1}$$\n",
    "Поэтому мы пройдем всего по $N-1$ значению каждого признака.\n",
    "\n",
    "Тогда решение на пне примет вид:\n",
    "$$(j_{opt}, t_{opt}) = \\arg\\min_{j,t} L \\left( B_{j, t}, X, y \\right)$$\n",
    "\n",
    "Для того чтобы рассчитать $loss$, необходимо еще одного прохождение по $N$, в результате получим, что полный алгоритм решающего пня будет выполняться за $O(DN^2)$, где $D$ - кол-во признаков, $N$ - кол-во объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdf79f-24d6-45c3-8da9-bc83e60fd11c",
   "metadata": {},
   "source": [
    "#### Главная проблема решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c4266-691a-4f03-b251-a7f1ed1839e8",
   "metadata": {},
   "source": [
    "Запустив предложенный выше алгоритм рещающего пня рекурсивно, он будет выполняться до тех пор, пока полностью не выучит обучающую выборку -> переобучится.\n",
    "\n",
    "Если мы поставим задачу найти оптимальное решающее дерево при минимальном количестве разбиений, то решение такой задачи не сможем найти за полиномиальное время, т. к. она относится к np - полным задачам.\n",
    "\n",
    "Чтобы решить ситуацию в настоящий момент пользуются двумя способами:\n",
    "1. Жадный алгоритм\n",
    "2. Оптимизация исходного алгоритма ассимптотически и в константу раз"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31885f32-3702-4f43-8d0e-e221f2c00a92",
   "metadata": {},
   "source": [
    "#### Жадный алгоритм *построения* решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379fe69-c61b-4e8e-9555-940c309d26fb",
   "metadata": {},
   "source": [
    "У нас уже есть матрица значений признаков $X$, определенная выше. Пусть $X_m \\subset X$ - множество всех объектов попавших в текущий лист.\n",
    "\n",
    "1. Создаем вершину $v$\n",
    "2. **Если**: выполнен ли критерий остановки $Stop(X_m)$, **то** останавливаемся и ставим ответ $Answ(X_m)$, объявив вершину листом.\n",
    "3. **Иначе**: Находим предикат $B_{j, t}$ имеющий лучшее разбиение на листы $X_m \\rightarrow X_l, X_r$. Максимизируя критерий ветвления $Branch(X_m)$\n",
    "4. Рекурсивно выполняем алгоритм для листьев $X_l, X_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e8292-6da5-405e-862a-3668c7274388",
   "metadata": {},
   "source": [
    "Подробнее разберем каждую функцию представленную в алгоритме:\n",
    "1. $Stop(X_m)$ - критерий остановки. Необходим для того, чтобы при построении решающего дерева мы могли остановиться и не переобучиться.\n",
    "2. $Answ(X_m)$ - функция вычисляющая ответ для листа, по попавшим в него объектам $X_m$. Может быть:\n",
    "   - Для задачи *классификации* ответ может быть: ***меткой самого частого класса*** или ***оценкой*** дискрет. ***распределения вероятностей классов*** для объектов в листе.\n",
    "   - Для задачи *регрессии*: ***средним, медианой или любой другой статистикой***\n",
    "   - Для любой задачи *простой моделью*: ***линейной функцией, синусойдой или любой другой функцией***\n",
    "3. $Stop(X_m)$ - критерий остановки при достижении определенных параметров дерево (про регуляризацию деревьев ниже)\n",
    "4. $Branch(X_m, j, t)$ - критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3502a-6a76-411d-9e92-c9b3eba3b999",
   "metadata": {},
   "source": [
    "##### Подробнее про критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a8bad-6467-4393-9925-65c9b60c02f1",
   "metadata": {},
   "source": [
    "Ответы дерева можем закодировать как: \n",
    "- $\\bar{c} \\in \\mathbb{R}^k$ для регрессии и классификации\n",
    "- $\\bar{c} = (c_1, ..., c_k) \\in \\mathbb{R}: \\sum_i \\bar{c} = 1$ для дискретного распределения вероятностей классов\n",
    "\n",
    "Предположим что задана функция потерь $L(y_i, c)$. *В момент поиска оптимального разделения* $X_m = X_l \\cup X_r$, мы можем *вычислить* для $X_m$ *константный таргет* $c$ (предикт дерева, если бы вершина была терминальной) и связанный с ним значение ф-ии потерь $L$. А именно - константа $c$ должна минимизировать среднее качество $L$.\n",
    "$$\\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "Тогда оптимальное значение:\n",
    "$$H(X_m) = \\min_{c \\in Y} \\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "$H(X_m)$ - называется ***неоднородностью (impurity)***, чем она ниже, тем предикт дерева ближе к *некоторому константному значению*.\n",
    "\n",
    "Таким же образом можно вычислить информативность всего решающего пня:\n",
    "1. $X_l$ - объекты попавшие в левый лист\n",
    "2. $X_r$ - объекты попавшие в правый лист\n",
    "3. $c_l$, $c_r$ - константы предсказаний в каждом листе для определенного $B_{j, t}(X_m)$\n",
    "\n",
    "Фукнция потерь всего пня:\n",
    "$$\\frac{1}{|X_m|} \\left( \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right)$$\n",
    "\n",
    "Связать это с информативностью:\n",
    "$$ \\frac{1}{|X_m|} \\left( \\frac{|X_l|}{|X_l|} \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\frac{|X_r|}{|X_r|} \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right) = $$\n",
    "$$ = \\frac{|X_l|}{|X_m|} H(X_l) + \\frac{|X_r|}{|X_m|} H(X_r) $$\n",
    "\n",
    "Чтобы получить качество разбиения мы найдем разницу информативностей вершины и получившихся листьев:\n",
    "$$Branch(X_m, j, t) = |X_m| * H(X_m) - |X_l| * H(X_l) - |X_r| * H(X_r)$$\n",
    "\n",
    "\n",
    "Функция потерь выбирается под конкретную задачу, подробнее про выбранные функции будет при их реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989d5af-407c-4db7-8617-7d1d5155e498",
   "metadata": {},
   "source": [
    "#### Регуляризация решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8da2d1-6867-4114-a213-1412557aae6f",
   "metadata": {},
   "source": [
    "Так как дерево обязательно переобучится, если его не ограничить, то необходимо упомянуть о методах регуляризации деревьев:\n",
    "1. Ограничения по максимальной глубине\n",
    "2. Ограничения на минимальное количество объектов в листе\n",
    "3. Ограничение на максимальное количество листьев в дереве\n",
    "4. Требование, чтобы функционал качества при делении текущей подвыборки на две улучшался не менее чем на $s\\%$\n",
    "\n",
    "Перечисленные действия возможно выполнить на разных этапах действия алгоритма:\n",
    "- Pre-pruning: В процессе построения дерева при достижении критерия остановки\n",
    "- Post-pruning: После построения дерева, удалить некоторые вершины так, чтобы качество упало не сильно. Проверяя качество на val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1888b2-4edd-4d0b-8403-c5f6f1e0c744",
   "metadata": {},
   "source": [
    "#### Функции потерь: классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fb2a0-6b87-433d-b5c8-be85d1c58e0c",
   "metadata": {},
   "source": [
    "##### Gini (Джини)\n",
    "\n",
    "Пусть предсказание модели - распределение вероятности классов: $\\bar{c} = (c_1, ..., c_k); \\sum_{i=1}^k c_i = 1$\n",
    "\n",
    "Тогда посчитаем *Brier score* на вероятностях классов при получившемся разбиении.\n",
    "$$BS(c) = \\frac{1}{N} \\sum_{i=1}^k \\left( c_k - \\mathbb{I} \\left[ y_i = k \\right] \\right)^2$$\n",
    "\n",
    "BS - при идеальном предсказании имеет значение. Чем меньше разница между предсказанным классом и вероятностью, тем ниже метрика.\n",
    "\n",
    "Следовательно функция неоднородности будет иметь вид минимизации функционала по каждому из классу\n",
    "\n",
    "$$H(X_m)= \\min_{\\sum_k c_k = 1} \\frac{1}{X_m} \\sum_{(x_i, y_i) \\in X_m} \\sum_{k=1}^K \\left( c_k - \\mathbb{I} \\left[ y_i = k \\right] \\right)^2$$\n",
    "\n",
    "Логично, что наименьшее зачение метрики достигается на $c$ состоящем из выборочных оценок частот классов в подвыборке $X_m$: $(p_1, ..., p_k), p_i = \\frac{1}{|X_m|} \\sum_i \\mathbb{I} \\left[ y_i = k \\right]$.\n",
    "\n",
    "Если подставить вектор выборочых оценок частот классов в форулу неоднородности, то получится свести задачу к следующему виду:\n",
    "$$H(X_m) = \\sum_{k=1}^K p_k (1 - p_k)$$\n",
    "\n",
    "Критерий Джини допускает следующую интерпретацию:\n",
    "\n",
    "$H(X_m)$ равно математическому ожиданию числа неправильных ответов, если мы будем присваивать им случайные метки классов согласно дискретному распределению заданному $(p_1, ..., p_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c067e0-0711-43c8-b2f8-ddc269ddf5ee",
   "metadata": {},
   "source": [
    "#### Реализация класса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86359b53-056a-4003-9cdf-e02db6e2147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "99275886-fc78-49c8-bd4e-31753c49ca0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log2, floor\n",
    "\n",
    "class SelfDecisionTreeClassifier():\n",
    "    '''\n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Criterion: Gini, LogLoss, Entropy\n",
    "\n",
    "    Regularization presented with:\n",
    "    - Max depth of tree\n",
    "    - Min_leaf_samples of tree\n",
    "    \n",
    "    The decisive stump predicate is loking best split on all features all values\n",
    "    So to optimize all calculations we will use PyTorch and GPU\n",
    "    '''\n",
    "    def __init__(self, device: str, criterion: str = 'gini', max_depth: int | None = None, min_leaf_samples: int | None = 1):\n",
    "        assert criterion in ('gini', 'entropy', 'log')\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_samples = min_leaf_samples\n",
    "        self.K = None\n",
    "        \n",
    "        self.__thresholds = []\n",
    "        self.__features = []\n",
    "        self.__impurity_list = []\n",
    "        self.__is_leaf = []\n",
    "        self.__split = []\n",
    "\n",
    "        self.__all_lists = [\n",
    "            self.__thresholds,\n",
    "            self.__features,\n",
    "            self.__impurity_list,\n",
    "            self.__is_leaf,\n",
    "            self.__split\n",
    "        ]\n",
    "        \n",
    "        self.N = 0\n",
    "        self.D = 0\n",
    "\n",
    "    def __get_childerns_idxs(self, cur_idx: int) -> tuple[int, int]:\n",
    "        return (2 * cur_idx + 1, 2 * cur_idx + 2)\n",
    "    \n",
    "    def __extend_lists(self, cur_idx: int) -> None:\n",
    "        _, r_idx = self.__get_childerns_idxs(cur_idx)\n",
    "        \n",
    "        for i in range(len(self.__all_lists)):\n",
    "            if len(self.__all_lists[i]) < r_idx:\n",
    "                self.__all_lists[i].extend([None] * (r_idx - len(self.__all_lists[i])))\n",
    "            \n",
    "    def __stop(self, cur_idx: int) -> bool:\n",
    "        if self.max_depth is not None:\n",
    "            if floor(log2(cur_idx + 1)) >= self.max_depth:\n",
    "                return True\n",
    "                \n",
    "        if self.min_leaf_samples is not None:\n",
    "            if sum(self.__split[cur_idx]) <= self.min_leaf_samples:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _gini_loss(self, indices: torch.LongTensor) -> float:\n",
    "        counts = torch.bincount(self.y[indices].flatten(), minlength=self.K)\n",
    "        probs = counts / indices.shape[0]\n",
    "        return torch.sum(probs * (1 - probs))\n",
    "\n",
    "    def _log_loss(self, indices: torch.LongTensor) -> float:\n",
    "        pass\n",
    "    \n",
    "    def _entropy_loss(self, indices: torch.LongTensor) -> float:\n",
    "        pass\n",
    "        \n",
    "    def __impurity(self, indices: torch.LongTensor) -> float:\n",
    "        loss_function = getattr(self, f'_{self.criterion}_loss')\n",
    "        return loss_function(indices)\n",
    "        \n",
    "    ## Recursive\n",
    "    def __split_node(self, cur_idx: int) -> None:\n",
    "        if self.__stop(cur_idx):\n",
    "            self.__is_leaf[cur_idx] = True\n",
    "            return\n",
    "\n",
    "        print('splitting')\n",
    "        best_branch_score = -float('inf') # Dict for all?\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "        best_Xr_idxs = None\n",
    "        best_Xl_idxs = None\n",
    "        best_Hr = None\n",
    "        best_Hl = None\n",
    "        flag = False\n",
    "\n",
    "        len_Xm = sum(self.__split[cur_idx])\n",
    "        Hm = self.__impurity_list[cur_idx] \n",
    "        \n",
    "        for feature_idx in range(self.D):\n",
    "            sorted_vals, indices = torch.sort(self.X[self.__split[cur_idx], feature_idx])\n",
    "            \n",
    "            prev_val = None       \n",
    "                 \n",
    "            for idx in range(0, indices.shape[0] - 1):\n",
    "                \n",
    "                cur_val = sorted_vals[idx]\n",
    "                if cur_val == prev_val:\n",
    "                    continue\n",
    "                \n",
    "                threshold = (self.X[indices[idx], feature_idx] + self.X[indices[idx + 1], feature_idx]) / 2\n",
    "                \n",
    "                Xr_idxs = indices[:idx + 1]\n",
    "                Xl_idxs = indices[idx + 1:]\n",
    "\n",
    "                Hr = self.__impurity(Xr_idxs) # Hr = H(Xr)\n",
    "                Hl = self.__impurity(Xl_idxs) # Hl = H(Xl)\n",
    "\n",
    "                branch_score = len_Xm * Hm - Xl_idxs.shape[0] * Hl - Xr_idxs.shape[0] * Hr\n",
    "\n",
    "                if branch_score > best_branch_score:\n",
    "                    best_branch_score = branch_score\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_Xr_idxs = Xr_idxs\n",
    "                    best_Xl_idxs = Xl_idxs\n",
    "                    best_Hr = Hr\n",
    "                    best_Hl = Hl                   \n",
    "                    flag = True\n",
    "                \n",
    "                prev_val = cur_val\n",
    "        \n",
    "        if not(flag):\n",
    "            self.__is_leaf[cur_idx] = True\n",
    "            return\n",
    "\n",
    "        print('Now at index: ', cur_idx)\n",
    "        print('Feature to split by: ', best_feature_idx)\n",
    "        print('Best founded threshold: ', best_threshold.item())\n",
    "        print('Best branch score: ', best_branch_score.item())\n",
    "        print('Element count in r_leaf, l_leaf: ', best_Xl_idxs.shape[0], best_Xr_idxs.shape[0])\n",
    "\n",
    "        ## заполнить массивы\n",
    "        self.__extend_lists(cur_idx)\n",
    "        self.__thresholds[cur_idx] = best_threshold\n",
    "        self.__features[cur_idx] = best_feature_idx\n",
    "        self.__is_leaf[cur_idx] = False\n",
    "        \n",
    "        r_idx, l_idx = self.__get_childerns_idxs(cur_idx)\n",
    "        self.__extend_lists(r_idx)\n",
    "        self.__extend_lists(l_idx)\n",
    "        \n",
    "        self.__impurity_list[r_idx] = best_Hr \n",
    "        mask_r = torch.zeros(self.N, dtype=torch.bool, device=self.device)\n",
    "        mask_r[best_Xr_idxs] = True\n",
    "        self.__split[r_idx] = mask_r\n",
    "\n",
    "        self.__impurity_list[l_idx] = best_Hl \n",
    "        mask_l = torch.zeros(self.N, dtype=torch.bool, device=self.device)\n",
    "        mask_l[best_Xl_idxs] = True\n",
    "        self.__split[l_idx] = mask_l\n",
    "\n",
    "        ## вызвать рекурсивно на листах\n",
    "        self.__split_node(r_idx)\n",
    "        self.__split_node(l_idx)         \n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        assert X.shape[0] == y.shape[0], 'N dimension of X and y must be equal'\n",
    "        \n",
    "        self.X = torch.tensor(X, device=self.device)\n",
    "        self.y = torch.tensor(y, device=self.device)\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.D = X.shape[1]\n",
    "\n",
    "        self.K = torch.unique(self.y).shape[0]\n",
    "        \n",
    "        self.__extend_lists(0)\n",
    "        self.__impurity_list[0] = self.__impurity(torch.nonzero(torch.ones_like(self.X[:, 0])))\n",
    "        self.__split[0] = [True] * self.X.shape[0]\n",
    "        self.__split_node(0)\n",
    "        \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3aa94746-5b7f-4f0c-b37e-4bdf32003b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[[0.10544537 0.06069044 0.0197     0.12296246 0.0223954  0.04970156]\n",
      " [0.11825319 0.32186868 0.04646249 0.13669859 0.0861182  0.15656569]\n",
      " [0.20714272 0.33595826 0.09152574 0.20791569 0.09108212 0.33733525]\n",
      " [0.34167183 0.36257827 0.24237871 0.4412436  0.13607636 0.36504722]\n",
      " [0.39693585 0.36475785 0.25599675 0.71660065 0.13643629 0.49471991]\n",
      " [0.52927625 0.42906917 0.59159674 0.73034698 0.15724544 0.52723778]\n",
      " [0.57441586 0.60236821 0.62893855 0.74217381 0.24804822 0.56745995]\n",
      " [0.58024921 0.64110213 0.77802963 0.82912735 0.45426842 0.62980429]\n",
      " [0.75436755 0.66803536 0.89193026 0.8359068  0.83422571 0.75748488]\n",
      " [0.95569921 0.69523764 0.92801013 0.85172019 0.92786705 0.91965314]]\n",
      "splitting\n",
      "Now at index:  0\n",
      "Feature to split by:  3\n",
      "Best founded threshold:  0.73626039852083\n",
      "Best branch score:  2.133333683013916\n",
      "Element count in r_leaf, l_leaf:  4 6\n",
      "splitting\n",
      "Now at index:  1\n",
      "Feature to split by:  2\n",
      "Best founded threshold:  0.834979945104366\n",
      "Best branch score:  2.6666665077209473\n",
      "Element count in r_leaf, l_leaf:  2 4\n",
      "splitting\n",
      "Now at index:  3\n",
      "Feature to split by:  1\n",
      "Best founded threshold:  0.5153068129605138\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  2 2\n",
      "splitting\n",
      "Now at index:  7\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.7650575311598378\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  1 1\n",
      "splitting\n",
      "Now at index:  8\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.7650575311598378\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  1 1\n",
      "splitting\n",
      "Now at index:  4\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.7650575311598378\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  1 1\n",
      "splitting\n",
      "Now at index:  2\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.6673083794431991\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  2 2\n",
      "splitting\n",
      "Now at index:  5\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.7650575311598378\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  1 1\n",
      "splitting\n",
      "Now at index:  6\n",
      "Feature to split by:  0\n",
      "Best founded threshold:  0.7650575311598378\n",
      "Best branch score:  0.0\n",
      "Element count in r_leaf, l_leaf:  1 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "random_array = np.random.rand(10, 6)\n",
    "print(np.sort(random_array, axis=0))\n",
    "SelfDecisionTreeClassifier(device, max_depth=4).fit(random_array, np.array([0, 1, 1, 0, 1, 1, 0, 1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0e216-e163-48e5-9e59-3d3154c9d61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
