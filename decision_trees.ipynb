{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e30cfa-0106-48e7-9c36-9570ed48636f",
   "metadata": {},
   "source": [
    "## Cамостоятельная реализация решающего дерева  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ef2f9-af35-49fa-9200-4e95cd28973b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Определение дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9bb34-c045-4e24-b5c5-622fb5ea66b8",
   "metadata": {},
   "source": [
    "Решающее дерево - **бинарное** дерево, в котором:\n",
    "1. Каждой *внутренней вершине* $v$ присвоен предикат предсказания: $B_v : \\mathbb{X} \\rightarrow \\{0, 1\\}$\n",
    "2. Каждой *листовой вершине* $v$ присвоен прогноз $C_v: \\mathbb{Y}$, где $\\mathbb{Y}$ - область значений таргета\n",
    "\n",
    "Каждый проход дерева начинается из корня. При прохождении очередной вершины мы двигаемся: *вправо*, если $B_v(x) = 1$; *влево*, если $B_v(x) = 0$. \n",
    "\n",
    "При достижении листа на объекте $x$, прогнозом для него будет являться $C_v$\n",
    "\n",
    "Особенности решающего дерева:\n",
    "1. Полученная функция кусочно-постоянная $\\rightarrow$ **не получится применить градиентные методы**\n",
    "2. Дерево **не может экстраполировать данные** за пределы уже имеющейся области значений признаков обучающей выборки\n",
    "3. Дереву свойственно **переобучение**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409d9db-851a-480c-85a8-74c0c9d5173c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Решающий пень"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da5e42-69fa-42ef-85e9-df71274f56b4",
   "metadata": {},
   "source": [
    "Дерево можно разбить на составляющие - решающие пни. Они будут представлять собой одну и вершину и два дочерних листа.\n",
    "\n",
    "Вершину мы будем разделять на листы при помощи предиката $B_{j, t}(x_i)$ . Качество разбиения мы будем оценивать при помощи критерия ветвления $Branch$.\n",
    "\n",
    "На листьях подзнее мы можем принять решение о необходимости дальнейшего разбиения -> построения еще одного решающего пня.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be38e5-e706-45b4-8cdb-ecb3d0bb4ed8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Сложность решающего пня"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbf3a7-862d-4783-9518-ceb94a548f54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Пусть у нас есть матрица значений признаков $X \\in \\mathbb{R}^{D \\times N}$ и вектор таргетов $Y \\in \\mathbb{R}^N$.\n",
    "\n",
    "В основе вершины пня будет находится разделяющий предикат:\n",
    "$$B_{j, t}(x_i) = \\mathbb{I}\\left[x_{ij} \\le t\\right]$$\n",
    "Мы будем проходить не по самим значениям признаков, а по средним между значениями.\n",
    "$$x_i < t_i \\le x_{i+1}$$\n",
    "Поэтому мы пройдем всего по $N-1$ значению каждого признака.\n",
    "\n",
    "Тогда решение на пне примет вид:\n",
    "$$(j_{opt}, t_{opt}) = \\arg\\min_{j,t} L \\left( B_{j, t}, X, y \\right)$$\n",
    "\n",
    "Для того чтобы рассчитать $loss$, необходимо еще одного прохождение по $N$, в результате получим, что полный алгоритм решающего пня будет выполняться за $O(DN^2)$, где $D$ - кол-во признаков, $N$ - кол-во объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdf79f-24d6-45c3-8da9-bc83e60fd11c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Главная проблема решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c4266-691a-4f03-b251-a7f1ed1839e8",
   "metadata": {},
   "source": [
    "Запустив предложенный выше алгоритм рещающего пня рекурсивно, он будет выполняться до тех пор, пока полностью не выучит обучающую выборку -> переобучится.\n",
    "\n",
    "Если мы поставим задачу найти оптимальное решающее дерево при минимальном количестве разбиений, то решение такой задачи не сможем найти за полиномиальное время, т. к. она относится к np - полным задачам.\n",
    "\n",
    "Чтобы решить ситуацию в настоящий момент пользуются двумя способами:\n",
    "1. Жадный алгоритм\n",
    "2. Оптимизация исходного алгоритма ассимптотически и в константу раз"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31885f32-3702-4f43-8d0e-e221f2c00a92",
   "metadata": {},
   "source": [
    "#### Жадный алгоритм *построения* решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379fe69-c61b-4e8e-9555-940c309d26fb",
   "metadata": {},
   "source": [
    "У нас уже есть матрица значений признаков $X$, определенная выше. Пусть $X_m \\subset X$ - множество всех объектов попавших в текущий лист.\n",
    "\n",
    "1. Создаем вершину $v$\n",
    "2. **Если**: выполнен ли критерий остановки $Stop(X_m)$, **то** останавливаемся и ставим ответ $Answ(X_m)$, объявив вершину листом.\n",
    "3. **Иначе**: Находим предикат $B_{j, t}$ имеющий лучшее разбиение на листы $X_m \\rightarrow X_l, X_r$. Максимизируя критерий ветвления $Branch(X_m)$\n",
    "4. Рекурсивно выполняем алгоритм для листьев $X_l, X_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e8292-6da5-405e-862a-3668c7274388",
   "metadata": {},
   "source": [
    "Подробнее разберем каждую функцию представленную в алгоритме:\n",
    "1. $Stop(X_m)$ - критерий остановки. Необходим для того, чтобы при построении решающего дерева мы могли остановиться и не переобучиться.\n",
    "2. $Answ(X_m)$ - функция вычисляющая ответ для листа, по попавшим в него объектам $X_m$. Может быть:\n",
    "   - Для задачи *классификации* ответ может быть: ***меткой самого частого класса*** или ***оценкой*** дискрет. ***распределения вероятностей классов*** для объектов в листе.\n",
    "   - Для задачи *регрессии*: ***средним, медианой или любой другой статистикой***\n",
    "   - Для любой задачи *простой моделью*: ***линейной функцией, синусойдой или любой другой функцией***\n",
    "3. $Stop(X_m)$ - критерий остановки при достижении определенных параметров дерево (про регуляризацию деревьев ниже)\n",
    "4. $Branch(X_m, j, t)$ - критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3502a-6a76-411d-9e92-c9b3eba3b999",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Подробнее про критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a8bad-6467-4393-9925-65c9b60c02f1",
   "metadata": {},
   "source": [
    "Ответы дерева можем закодировать как: \n",
    "- $\\bar{c} \\in \\mathbb{R}^k$ для регрессии и классификации\n",
    "- $\\bar{c} = (c_1, ..., c_k) \\in \\mathbb{R}: \\sum_i \\bar{c} = 1$ для дискретного распределения вероятностей классов\n",
    "\n",
    "Предположим что задана функция потерь $L(y_i, c)$. *В момент поиска оптимального разделения* $X_m = X_l \\cup X_r$, мы можем *вычислить* для $X_m$ *константный таргет* $c$ (предикт дерева, если бы вершина была терминальной) и связанный с ним значение ф-ии потерь $L$. А именно - константа $c$ должна минимизировать среднее качество $L$.\n",
    "$$\\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "Тогда оптимальное значение:\n",
    "$$H(X_m) = \\min_{c \\in Y} \\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "$H(X_m)$ - называется ***информативностью (impurity)***, чем она ниже, тем предикт дерева ближе к *некоторому константному значению*.\n",
    "\n",
    "Таким же образом можно вычислить информативность всего решающего пня:\n",
    "1. $X_l$ - объекты попавшие в левый лист\n",
    "2. $X_r$ - объекты попавшие в правый лист\n",
    "3. $c_l$, $c_r$ - константы предсказаний в каждом листе для определенного $B_{j, t}(X_m)$\n",
    "\n",
    "Фукнция потерь всего пня:\n",
    "$$\\frac{1}{|X_m|} \\left( \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right)$$\n",
    "\n",
    "Связать это с информативностью:\n",
    "$$ \\frac{1}{|X_m|} \\left( \\frac{|X_l|}{|X_l|} \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\frac{|X_r|}{|X_r|} \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right) = $$\n",
    "$$ = \\frac{|X_l|}{|X_m|} H(X_l) + \\frac{|X_r|}{|X_m|} H(X_r) $$\n",
    "\n",
    "Чтобы получить качество разбиения мы найдем разницу информативностей вершины и получившихся листьев:\n",
    "$$Branch(X_m, j, t) = |X_m| * H(X_m) - |X_l| * H(X_l) - |X_r| * H(X_r)$$\n",
    "\n",
    "\n",
    "Функция потерь выбирается под конкретную задачу, подробнее про выбранные функции будет при их реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989d5af-407c-4db7-8617-7d1d5155e498",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Регуляризация решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8da2d1-6867-4114-a213-1412557aae6f",
   "metadata": {},
   "source": [
    "Так как дерево обязательно переобучится, если его не ограничить, то необходимо упомянуть о методах регуляризации деревьев:\n",
    "1. Ограничения по максимальной глубине\n",
    "2. Ограничения на минимальное количество объектов в листе\n",
    "3. Ограничение на максимальное количество листьев в дереве\n",
    "4. Требование, чтобы функционал качества при делении текущей подвыборки на две улучшался не менее чем на $s\\%$\n",
    "\n",
    "Перечисленные действия возможно выполнить на разных этапах действия алгоритма:\n",
    "- Pre-pruning: В процессе построения дерева при достижении критерия остановки\n",
    "- Post-pruning: После построения дерева, удалить некоторые вершины так, чтобы качество упало не сильно. Проверяя качество на val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c067e0-0711-43c8-b2f8-ddc269ddf5ee",
   "metadata": {},
   "source": [
    "#### Реализация класса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99275886-fc78-49c8-bd4e-31753c49ca0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSelfDecisionTreeClassifier\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m'''\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[33;43;03m    Decision Tree Classifier\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;43;03m    So to optimize all calculations we will use PyTorch and GPU\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[33;43;03m    '''\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_leaf_samples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mSelfDecisionTreeClassifier\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__split_node\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: \u001b[43mnp\u001b[49m.ndarray, y: np.ndarray):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.X = torch.tensor(X, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.y = torch.tensor(y, device=\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfDecisionTreeClassifier():\n",
    "    '''\n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Criterion: Gini\n",
    "\n",
    "    Regularization presented with:\n",
    "    - Max depth of tree\n",
    "    - Min_leaf_samples of tree\n",
    "\n",
    "    \n",
    "    The decisive stump predicate is loking best split on all features all values\n",
    "    So to optimize all calculations we will use PyTorch and GPU\n",
    "    '''\n",
    "    def __init__(self, device: str, criterion: str = 'gini', max_depth: int | None = None, min_leaf_samples: int | None = 1):\n",
    "        assert criterion in ('gini', 'entropy', 'log_loss')\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_samples = min_leaf_samples\n",
    "\n",
    "        self.num_classes = 0\n",
    "        self.__thresholds = []\n",
    "        self.__features = []\n",
    "        self.__is_leaf = []\n",
    "        \n",
    "        self.N = 0\n",
    "        self.D = 0\n",
    "        \n",
    "\n",
    "    def __stop(self):\n",
    "        pass\n",
    "\n",
    "    def __gini_loss(self):\n",
    "        pass\n",
    "        \n",
    "    def __impurity(self):\n",
    "        pass\n",
    "\n",
    "    ## Recursive\n",
    "    def __split_node(self, cur_idx: int):\n",
    "        if self.__stop():\n",
    "            self.__is_leaf[cur_idx] = True\n",
    "            return\n",
    "        \n",
    "        best_score = -float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_idx in range(D):\n",
    "            \n",
    "            sorted_vals, indices = torch.sort(self.X[:feature_idx])\n",
    "            unique_vals = torch.unique(sorted_vals)\n",
    "\n",
    "            #continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        assert X.shape[0] == y.shape[0], 'N dimension of X and y must be equal'\n",
    "        \n",
    "        self.X = torch.tensor(X, device=self.device)\n",
    "        self.y = torch.tensor(y, device=self.device)\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.D = X.shape[1]\n",
    "\n",
    "        self.num_classes = torch.unique(self.y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa94746-5b7f-4f0c-b37e-4bdf32003b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
