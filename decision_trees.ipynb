{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e30cfa-0106-48e7-9c36-9570ed48636f",
   "metadata": {},
   "source": [
    "## Cамостоятельная реализация решающего дерева  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ef2f9-af35-49fa-9200-4e95cd28973b",
   "metadata": {},
   "source": [
    "#### Определение дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9bb34-c045-4e24-b5c5-622fb5ea66b8",
   "metadata": {},
   "source": [
    "Решающее дерево - **бинарное** дерево, в котором:\n",
    "1. Каждой *внутренней вершине* $v$ присвоен предикат предсказания: $B_v : \\mathbb{X} \\rightarrow \\{0, 1\\}$\n",
    "2. Каждой *листовой вершине* $v$ присвоен прогноз $C_v: \\mathbb{Y}$, где $\\mathbb{Y}$ - область значений таргета\n",
    "\n",
    "Каждый проход дерева начинается из корня. При прохождении очередной вершины мы двигаемся: *вправо*, если $B_v(x) = 1$; *влево*, если $B_v(x) = 0$. \n",
    "\n",
    "При достижении листа на объекте $x$, прогнозом для него будет являться $C_v$\n",
    "\n",
    "Особенности решающего дерева:\n",
    "1. Полученная функция кусочно-постоянная $\\rightarrow$ **не получится применить градиентные методы**\n",
    "2. Дерево **не может экстраполировать данные** за пределы уже имеющейся области значений признаков обучающей выборки\n",
    "3. Дереву свойственно **переобучение**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409d9db-851a-480c-85a8-74c0c9d5173c",
   "metadata": {},
   "source": [
    "#### Решающий пень"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da5e42-69fa-42ef-85e9-df71274f56b4",
   "metadata": {},
   "source": [
    "Дерево можно разбить на составляющие - решающие пни. Они будут представлять собой одну и вершину и два дочерних листа.\n",
    "\n",
    "Вершину мы будем разделять на листы при помощи предиката $B_{j, t}(x_i)$ . Качество разбиения мы будем оценивать при помощи критерия ветвления $Branch$.\n",
    "\n",
    "На листьях подзнее мы можем принять решение о необходимости дальнейшего разбиения -> построения еще одного решающего пня.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be38e5-e706-45b4-8cdb-ecb3d0bb4ed8",
   "metadata": {},
   "source": [
    "#### Сложность решающего пня"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbf3a7-862d-4783-9518-ceb94a548f54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Пусть у нас есть матрица значений признаков $X \\in \\mathbb{R}^{D \\times N}$ и вектор таргетов $Y \\in \\mathbb{R}^N$.\n",
    "\n",
    "В основе вершины пня будет находится разделяющий предикат:\n",
    "$$B_{j, t}(x_i) = \\mathbb{I}\\left[x_{ij} \\le t\\right]$$\n",
    "Мы будем проходить не по самим значениям признаков, а по средним между значениями.\n",
    "$$x_i < t_i \\le x_{i+1}$$\n",
    "Поэтому мы пройдем всего по $N-1$ значению каждого признака.\n",
    "\n",
    "Тогда решение на пне примет вид:\n",
    "$$(j_{opt}, t_{opt}) = \\arg\\min_{j,t} L \\left( B_{j, t}, X, y \\right)$$\n",
    "\n",
    "Для того чтобы рассчитать $loss$, необходимо еще одного прохождение по $N$, в результате получим, что полный алгоритм решающего пня будет выполняться за $O(DN^2)$, где $D$ - кол-во признаков, $N$ - кол-во объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdf79f-24d6-45c3-8da9-bc83e60fd11c",
   "metadata": {},
   "source": [
    "#### Главная проблема решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c4266-691a-4f03-b251-a7f1ed1839e8",
   "metadata": {},
   "source": [
    "Запустив предложенный выше алгоритм рещающего пня рекурсивно, он будет выполняться до тех пор, пока полностью не выучит обучающую выборку -> переобучится.\n",
    "\n",
    "Если мы поставим задачу найти оптимальное решающее дерево при минимальном количестве разбиений, то решение такой задачи не сможем найти за полиномиальное время, т. к. она относится к np - полным задачам.\n",
    "\n",
    "Чтобы решить ситуацию в настоящий момент пользуются двумя способами:\n",
    "1. Жадный алгоритм\n",
    "2. Оптимизация исходного алгоритма ассимптотически и в константу раз"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31885f32-3702-4f43-8d0e-e221f2c00a92",
   "metadata": {},
   "source": [
    "#### Жадный алгоритм *построения* решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379fe69-c61b-4e8e-9555-940c309d26fb",
   "metadata": {},
   "source": [
    "У нас уже есть матрица значений признаков $X$, определенная выше. Пусть $X_m \\subset X$ - множество всех объектов попавших в текущий лист.\n",
    "\n",
    "1. Создаем вершину $v$\n",
    "2. **Если**: выполнен ли критерий остановки $Stop(X_m)$, **то** останавливаемся и ставим ответ $Answ(X_m)$, объявив вершину листом.\n",
    "3. **Иначе**: Находим предикат $B_{j, t}$ имеющий лучшее разбиение на листы $X_m \\rightarrow X_l, X_r$. Максимизируя критерий ветвления $Branch(X_m)$\n",
    "4. Рекурсивно выполняем алгоритм для листьев $X_l, X_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e8292-6da5-405e-862a-3668c7274388",
   "metadata": {},
   "source": [
    "Подробнее разберем каждую функцию представленную в алгоритме:\n",
    "1. $Stop(X_m)$ - критерий остановки. Необходим для того, чтобы при построении решающего дерева мы могли остановиться и не переобучиться.\n",
    "2. $Answ(X_m)$ - функция вычисляющая ответ для листа, по попавшим в него объектам $X_m$. Может быть:\n",
    "   - Для задачи *классификации* ответ может быть: ***меткой самого частого класса*** или ***оценкой*** дискрет. ***распределения вероятностей классов*** для объектов в листе.\n",
    "   - Для задачи *регрессии*: ***средним, медианой или любой другой статистикой***\n",
    "   - Для любой задачи *простой моделью*: ***линейной функцией, синусойдой или любой другой функцией***\n",
    "3. $Stop(X_m)$ - критерий остановки при достижении определенных параметров дерево (про регуляризацию деревьев ниже)\n",
    "4. $Branch(X_m, j, t)$ - критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3502a-6a76-411d-9e92-c9b3eba3b999",
   "metadata": {},
   "source": [
    "##### Подробнее про критерий ветвления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a8bad-6467-4393-9925-65c9b60c02f1",
   "metadata": {},
   "source": [
    "Ответы дерева можем закодировать как: \n",
    "- $\\bar{c} \\in \\mathbb{R}^k$ для регрессии и классификации\n",
    "- $\\bar{c} = (c_1, ..., c_k) \\in \\mathbb{R}: \\sum_i \\bar{c} = 1$ для дискретного распределения вероятностей классов\n",
    "\n",
    "Предположим что задана функция потерь $L(y_i, c)$. *В момент поиска оптимального разделения* $X_m = X_l \\cup X_r$, мы можем *вычислить* для $X_m$ *константный таргет* $c$ (предикт дерева, если бы вершина была терминальной) и связанный с ним значение ф-ии потерь $L$. А именно - константа $c$ должна минимизировать среднее качество $L$.\n",
    "$$\\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "Тогда оптимальное значение:\n",
    "$$H(X_m) = \\min_{c \\in Y} \\frac{1}{|X_m|} \\sum_{(x_i, y_i) \\in X_m} L(y_i, c)$$\n",
    "\n",
    "$H(X_m)$ - называется ***неоднородностью (impurity)***, чем она ниже, тем предикт дерева ближе к *некоторому константному значению*.\n",
    "\n",
    "Таким же образом можно вычислить информативность всего решающего пня:\n",
    "1. $X_l$ - объекты попавшие в левый лист\n",
    "2. $X_r$ - объекты попавшие в правый лист\n",
    "3. $c_l$, $c_r$ - константы предсказаний в каждом листе для определенного $B_{j, t}(X_m)$\n",
    "\n",
    "Фукнция потерь всего пня:\n",
    "$$\\frac{1}{|X_m|} \\left( \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right)$$\n",
    "\n",
    "Связать это с информативностью:\n",
    "$$ \\frac{1}{|X_m|} \\left( \\frac{|X_l|}{|X_l|} \\sum_{(x_i, y_i) \\in X_l} L(y_i, c_l) + \\frac{|X_r|}{|X_r|} \\sum_{(x_i, y_i) \\in X_r} L(y_i, c_r) \\right) = $$\n",
    "$$ = \\frac{|X_l|}{|X_m|} H(X_l) + \\frac{|X_r|}{|X_m|} H(X_r) $$\n",
    "\n",
    "Чтобы получить качество разбиения мы найдем разницу информативностей вершины и получившихся листьев:\n",
    "$$Branch(X_m, j, t) = |X_m| * H(X_m) - |X_l| * H(X_l) - |X_r| * H(X_r)$$\n",
    "\n",
    "\n",
    "Функция потерь выбирается под конкретную задачу, подробнее про выбранные функции будет при их реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989d5af-407c-4db7-8617-7d1d5155e498",
   "metadata": {},
   "source": [
    "#### Регуляризация решающих деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8da2d1-6867-4114-a213-1412557aae6f",
   "metadata": {},
   "source": [
    "Так как дерево обязательно переобучится, если его не ограничить, то необходимо упомянуть о методах регуляризации деревьев:\n",
    "1. Ограничения по максимальной глубине\n",
    "2. Ограничения на минимальное количество объектов в листе\n",
    "3. Ограничение на максимальное количество листьев в дереве\n",
    "4. Требование, чтобы функционал качества при делении текущей подвыборки на две улучшался не менее чем на $s\\%$\n",
    "\n",
    "Перечисленные действия возможно выполнить на разных этапах действия алгоритма:\n",
    "- Pre-pruning: В процессе построения дерева при достижении критерия остановки\n",
    "- Post-pruning: После построения дерева, удалить некоторые вершины так, чтобы качество упало не сильно. Проверяя качество на val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1888b2-4edd-4d0b-8403-c5f6f1e0c744",
   "metadata": {},
   "source": [
    "#### Функции потерь: классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fb2a0-6b87-433d-b5c8-be85d1c58e0c",
   "metadata": {},
   "source": [
    "##### Gini (Джини)\n",
    "\n",
    "Пусть предсказание модели - распределение вероятности классов: $\\bar{c} = (c_1, ..., c_k); \\sum_{i=1}^k c_i = 1$\n",
    "\n",
    "Тогда посчитаем *Brier score* на вероятностях классов при получившемся разбиении.\n",
    "$$BS(c) = \\frac{1}{N} \\sum_{i=1}^k \\left( c_k - \\mathbb{I} \\left[ y_i = k \\right] \\right)^2$$\n",
    "\n",
    "BS - при идеальном предсказании имеет значение. Чем меньше разница между предсказанным классом и вероятностью, тем ниже метрика.\n",
    "\n",
    "Следовательно функция неоднородности будет иметь вид минимизации функционала по каждому из классу\n",
    "\n",
    "$$H(X_m)= \\min_{\\sum_k c_k = 1} \\frac{1}{X_m} \\sum_{(x_i, y_i) \\in X_m} \\sum_{k=1}^K \\left( c_k - \\mathbb{I} \\left[ y_i = k \\right] \\right)^2$$\n",
    "\n",
    "Логично, что наименьшее зачение метрики достигается на $c$ состоящем из выборочных оценок частот классов в подвыборке $X_m$: $(p_1, ..., p_k), p_i = \\frac{1}{|X_m|} \\sum_i \\mathbb{I} \\left[ y_i = k \\right]$.\n",
    "\n",
    "Если подставить вектор выборочых оценок частот классов в форулу неоднородности, то получится свести задачу к следующему виду:\n",
    "$$H(X_m) = \\sum_{k=1}^K p_k (1 - p_k)$$\n",
    "$$H(X_m) = 1 - \\sum_{k=1}^K p_k^2 $$\n",
    "Критерий Джини допускает следующую интерпретацию:\n",
    "\n",
    "$H(X_m)$ равно математическому ожиданию числа неправильных ответов, если мы будем присваивать им случайные метки классов согласно дискретному распределению заданному $(p_1, ..., p_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c067e0-0711-43c8-b2f8-ddc269ddf5ee",
   "metadata": {},
   "source": [
    "#### Реализация класса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30400a9a-efb9-4a6d-8d38-6e77e2c751b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: rewrite fit method with BFS APPROACH\n",
    "## TODO: SORT FEATURES ONE TIME before fit\n",
    "## TODO: find split with VECTORIZED CUMSUM\n",
    "## TODO: predict with OPTIMIZED BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86359b53-056a-4003-9cdf-e02db6e2147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99275886-fc78-49c8-bd4e-31753c49ca0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log2, floor\n",
    "\n",
    "class SelfDecisionTreeClassifier():\n",
    "    '''\n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Criterion: Gini, LogLoss, Entropy\n",
    "\n",
    "    Regularization presented with:\n",
    "    - Max depth of tree\n",
    "    - Min_leaf_samples of tree\n",
    "    \n",
    "    Tree is building with DFS approach (# To optimize can use BFS)\n",
    "    The decisive stump predicate is loking best split on all features all values\n",
    "    So to partially optimize calculations we will use PyTorch and GPU\n",
    "    '''\n",
    "    def __init__(self, device: str, criterion: str = 'gini', max_depth: int | None = None, min_leaf_samples: int | None = 1):\n",
    "        assert criterion in ('gini', 'entropy', 'log')\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_samples = min_leaf_samples\n",
    "        self.K = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        self.__thresholds = []\n",
    "        self.__features = []\n",
    "        self.__impurity_list = []\n",
    "        self.__is_leaf = []\n",
    "        self.__split = []\n",
    "        self.__probs = []\n",
    "\n",
    "        self.__all_lists = [\n",
    "            self.__thresholds,\n",
    "            self.__features,\n",
    "            self.__impurity_list,\n",
    "            self.__is_leaf,\n",
    "            self.__split,\n",
    "            self.__probs\n",
    "        ]\n",
    "        \n",
    "        self.N = 0\n",
    "        self.D = 0\n",
    "\n",
    "    def __get_childerns_idxs(self, cur_idx: int) -> tuple[int, int]:\n",
    "        return (2 * cur_idx + 1, 2 * cur_idx + 2)\n",
    "    \n",
    "    def __extend_lists(self, cur_idx: int) -> None:\n",
    "        _, r_idx = self.__get_childerns_idxs(cur_idx)\n",
    "        \n",
    "        for i in range(len(self.__all_lists)):\n",
    "            if len(self.__all_lists[i]) < r_idx:\n",
    "                self.__all_lists[i].extend([None] * (r_idx - len(self.__all_lists[i])))\n",
    "            \n",
    "    def __stop(self, cur_idx: int) -> bool:\n",
    "        if self.max_depth is not None:\n",
    "            if floor(log2(cur_idx + 1)) >= self.max_depth:\n",
    "                return True\n",
    "                \n",
    "        if self.min_leaf_samples is not None:\n",
    "            if self.__split[cur_idx].sum().item() <= self.min_leaf_samples:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __calc_probs(self, indices: torch.LongTensor | torch.Tensor) -> torch.Tensor:\n",
    "        return torch.bincount(self.y[indices].flatten(), minlength=self.K) / indices.shape[0]\n",
    "    \n",
    "    def _gini_loss(self, indices: torch.LongTensor) -> float:\n",
    "        probs = self.__calc_probs(indices)\n",
    "        # return torch.sum(probs * (1 - probs))\n",
    "        return 1.0 - torch.sum(probs ** 2)\n",
    "\n",
    "    def _log_loss(self, indices: torch.LongTensor) -> float:\n",
    "        pass\n",
    "    \n",
    "    def _entropy_loss(self, indices: torch.LongTensor) -> float:\n",
    "        pass\n",
    "        \n",
    "    def __impurity(self, indices: torch.LongTensor) -> float:\n",
    "        loss_function = getattr(self, f'_{self.criterion}_loss')\n",
    "        return loss_function(indices)\n",
    "\n",
    "    def __create_leaf(self, cur_idx: int) -> None:\n",
    "        self.__is_leaf[cur_idx] = True\n",
    "        self.__probs[cur_idx] = self.__calc_probs(torch.where(self.__split[cur_idx])[0])\n",
    "\n",
    "        # print('---' * 4, ' LEAF ', '---' * 4,)\n",
    "        # print('Now at index: ', cur_idx)\n",
    "        # print('Obj count: ', self.__split[cur_idx].sum().item())\n",
    "        # print('Probs: ', self.__probs[cur_idx])\n",
    "        # print('---' * 11, end='\\n\\n')\n",
    "        \n",
    "        return \n",
    "        \n",
    "    ## Recursive\n",
    "    def __split_node(self, cur_idx: int) -> None:\n",
    "        if self.__stop(cur_idx):\n",
    "            self.__create_leaf(cur_idx)\n",
    "            return\n",
    "        \n",
    "        best_branch_score = -float('inf') # Dict for all?\n",
    "        best_feature_idx = None\n",
    "        best_threshold = None\n",
    "        best_Xr_idxs = None\n",
    "        best_Xl_idxs = None\n",
    "        best_Hr = None\n",
    "        best_Hl = None\n",
    "        flag = False\n",
    "\n",
    "        len_Xm = self.__split[cur_idx].sum().item()\n",
    "        Hm = self.__impurity_list[cur_idx] \n",
    "        \n",
    "        for feature_idx in range(self.D):\n",
    "            \n",
    "            global_indices = torch.where(self.__split[cur_idx])[0]\n",
    "            sorted_vals, order = torch.sort(self.X[global_indices, feature_idx])\n",
    "            sorted_global = global_indices[order]\n",
    "            \n",
    "            prev_val = None       \n",
    "                 \n",
    "            for idx in range(0, order.shape[0] - 1):\n",
    "                \n",
    "                cur_val = sorted_vals[idx]\n",
    "                if cur_val == prev_val:\n",
    "                    continue\n",
    "                \n",
    "                threshold = (self.X[order[idx], feature_idx] + self.X[order[idx + 1], feature_idx]) / 2\n",
    "                \n",
    "                Xr_idxs = sorted_global[:idx + 1]\n",
    "                Xl_idxs = sorted_global[idx + 1:]\n",
    "\n",
    "                Hr = self.__impurity(Xr_idxs) # Hr = H(Xr)\n",
    "                Hl = self.__impurity(Xl_idxs) # Hl = H(Xl)\n",
    "\n",
    "                branch_score = Hm - (Xl_idxs.shape[0]/len_Xm) * Hl - (Xr_idxs.shape[0]/len_Xm) * Hr\n",
    "\n",
    "                if branch_score > best_branch_score:\n",
    "                    best_branch_score = branch_score\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_Xr_idxs = Xr_idxs\n",
    "                    best_Xl_idxs = Xl_idxs\n",
    "                    best_Hr = Hr\n",
    "                    best_Hl = Hl                   \n",
    "                    flag = True\n",
    "                \n",
    "                prev_val = cur_val\n",
    "        \n",
    "        if not(flag):\n",
    "            self.__create_leaf(cur_idx)\n",
    "            return\n",
    "\n",
    "        # print('NODE')\n",
    "        # print('Now at index: ', cur_idx)\n",
    "        # print('Main node of stump objects count: ', self.__split[cur_idx].sum().item())\n",
    "        # print('Feature to split by: ', best_feature_idx)\n",
    "        # print('Best founded threshold: ', best_threshold.item())\n",
    "        # print('Best branch score: ', best_branch_score.item())\n",
    "        # print('Element count in r_leaf, l_leaf: ', best_Xl_idxs.shape[0], best_Xr_idxs.shape[0], end='\\n\\n')\n",
    "\n",
    "        ## заполнить массивы\n",
    "        self.__extend_lists(cur_idx)\n",
    "        self.__thresholds[cur_idx] = best_threshold\n",
    "        self.__features[cur_idx] = best_feature_idx\n",
    "        self.__is_leaf[cur_idx] = False\n",
    "        \n",
    "        r_idx, l_idx = self.__get_childerns_idxs(cur_idx)\n",
    "        self.__extend_lists(r_idx)\n",
    "        self.__extend_lists(l_idx)\n",
    "        \n",
    "        self.__impurity_list[r_idx] = best_Hr \n",
    "        mask_r = torch.zeros(self.N, dtype=torch.bool, device=self.device)\n",
    "        mask_r[best_Xr_idxs] = True\n",
    "        self.__split[r_idx] = mask_r\n",
    "\n",
    "        self.__impurity_list[l_idx] = best_Hl \n",
    "        mask_l = torch.zeros(self.N, dtype=torch.bool, device=self.device)\n",
    "        mask_l[best_Xl_idxs] = True\n",
    "        self.__split[l_idx] = mask_l\n",
    "\n",
    "        ## вызвать рекурсивно на листах\n",
    "        self.__split_node(r_idx)\n",
    "        self.__split_node(l_idx)         \n",
    "\n",
    "    \n",
    "    def __predict_single(self, X: torch.tensor) -> float:\n",
    "        idx = 0\n",
    "        while not(self.__is_leaf[idx]):\n",
    "            feat = self.__features[idx]\n",
    "            thr = self.__thresholds[idx]\n",
    "            if X[feat] >= thr:\n",
    "                idx = 2 * idx + 2\n",
    "            else:\n",
    "                idx = 2 * idx + 1\n",
    "        return self.__probs[idx].cpu().numpy()\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        assert X.shape[0] == y.shape[0], 'N dimension of X and y must be equal'\n",
    "        \n",
    "        self.X = torch.tensor(X, device=self.device)\n",
    "        # _, self.sorted_indices = torch.sort(self.X, dim=1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long, device=self.device).flatten()\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.D = X.shape[1]\n",
    "\n",
    "        self.K = torch.unique(self.y).shape[0]\n",
    "        \n",
    "        self.__extend_lists(0)\n",
    "        self.__impurity_list[0] = self.__impurity(torch.arange(self.N, device=self.device))\n",
    "        self.__split[0] = torch.ones_like(self.y, dtype=bool, device=self.device)\n",
    "        \n",
    "        self.__split_node(0)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> list[torch.Tensor]:\n",
    "        assert self.is_fitted == True, 'You must fit before predict'\n",
    "        assert X.shape[1] == self.X.shape[1], 'Number of features must be equal fitted number of features'\n",
    "        \n",
    "        X = torch.tensor(X, dtype=float, device=self.device)\n",
    "        probs = []\n",
    "        for i in range(X.shape[0]):\n",
    "            probs.append(self.__predict_single(X[i, :]))\n",
    "        return probs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aa94746-5b7f-4f0c-b37e-4bdf32003b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "# print(np.sort(random_array, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a78e74c-9dd8-499f-abe1-ab38ab9b4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684a9852-2730-40c2-9df7-54dfdba7196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 6)\n",
    "y = np.random.randint(0, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fd40a9a-27fa-4530-b23b-b6f97ecc25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_classifier = DecisionTreeClassifier(max_depth=4, min_samples_leaf=2)\n",
    "sk_classifier.fit(X, y)\n",
    "pred_proba_sk = sk_classifier.predict_proba(X)\n",
    "sk_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in pred_proba_sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c498f9c-6c70-4541-9618-0e161a8cfbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self_classifier = SelfDecisionTreeClassifier(device, max_depth=4, min_leaf_samples=2)\n",
    "self_classifier.fit(X, y)\n",
    "pred_probs_self = np.array(self_classifier.predict(X))\n",
    "self_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in pred_probs_self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bbc9b9e-6af0-4972-9357-06f296888d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self tree f1:  0.6137071651090342\n",
      "Sklearn tree f1:  0.7634069400630915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Self tree f1: ', f1_score(y, self_pred_labels))\n",
    "print('Sklearn tree f1: ', f1_score(y, sk_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c8ea4-2d25-41ca-b85c-86216cc54cf3",
   "metadata": {},
   "source": [
    "### Использование на данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d0d4fdc-1f83-4b04-a788-bbb722b1eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26e70839-31a8-4dda-a732-2527880d3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/train.npy')\n",
    "X, y = data[:, 1:], data[:, :1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c39d2421-b17d-4525-a07b-9fda73f56dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(711, 9) (711, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab26a667-d8c6-402c-be10-8aaad4d049b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 9) (178, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8859edd-738e-4626-aa6c-eb6fdaa3a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEAN tree\n",
      "Train score:  0.7966457023060797\n",
      "Test score:  0.6890756302521008\n"
     ]
    }
   ],
   "source": [
    "print('SKLEAN tree')\n",
    "sk_tree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=3)\n",
    "sk_tree.fit(X_train, y_train)\n",
    "\n",
    "train_proba_sk = sk_tree.predict_proba(X_train)\n",
    "train_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in train_proba_sk]\n",
    "print('Train score: ', f1_score(train_pred_labels, y_train))\n",
    "\n",
    "pred_proba_sk = sk_tree.predict_proba(X_test)\n",
    "sk_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in pred_proba_sk]\n",
    "print('Test score: ', f1_score(sk_pred_labels, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22e0bec3-56cc-45df-9a02-dedd7a1fb178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF tree\n",
      "Train score:  0.6385372714486639\n",
      "Test score:  0.6483516483516484\n"
     ]
    }
   ],
   "source": [
    "print('SELF tree')\n",
    "self_tree = SelfDecisionTreeClassifier(device, max_depth=6, min_leaf_samples=3)\n",
    "self_tree.fit(X_train, y_train)\n",
    "\n",
    "train_proba_self = self_tree.predict(X_train)\n",
    "train_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in train_proba_self]\n",
    "print('Train score: ', f1_score(train_pred_labels, y_train))\n",
    "\n",
    "pred_probs_self = self_tree.predict(X_test)\n",
    "self_pred_labels = [0 if arr[0] > 0.5 else 1 for arr in pred_probs_self]\n",
    "print('Test score: ', f1_score(self_pred_labels, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff606a-2de8-41bd-ac54-c94d76886459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
